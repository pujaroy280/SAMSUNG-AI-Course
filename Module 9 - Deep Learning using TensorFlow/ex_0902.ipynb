{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise #0902"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convolutional Neural Network (color images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the codes in TensorFlow 2.0 without editing those in version 1.x (except for the contrib module).\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets.cifar10 import load_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Download the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about the dataset can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "n_train_size = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Take a look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images already reshaped as 32x32.\n",
    "# 3 Color channels.\n",
    "# y is not one-hot-encoded yet.\n",
    "print(\"Training data X shape: {}\".format(X_train.shape))\n",
    "print(\"Training data y shape: {}\".format(y_train.shape))\n",
    "print(\"\\n\")\n",
    "print(\"Testing data X shape: {}\".format(X_test.shape))\n",
    "print(\"Testing data y shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_image= 123                                                        # Image index. You can change it at will.\n",
    "a_single_image= X_train[i_image,:,:,:]\n",
    "plt.imshow(a_single_image)                                          #  Display as a color image.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the minimum and maximum pixel value.\n",
    "print(\"MIN : {}\".format(a_single_image.min()))                 \n",
    "print(\"MAX : {}\".format(a_single_image.max())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Data preprocessing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling.\n",
    "X_train = X_train/255                      \n",
    "X_test = X_test/255                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot-Encoding\n",
    "y = np.concatenate([y_train[:,0],y_test[:,0]],axis=0)\n",
    "y = np.array(pd.get_dummies(y, drop_first=False))               # drop_frist = False for one-hot-encoding\n",
    "y_train = y[:n_train_size,:]\n",
    "y_test = y[n_train_size:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Define the hyperparameters and placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "n_epochs  = 50001\n",
    "learn_rate = 0.0001\n",
    "drop_prob = 0.5                                     # For the dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ph = tf.placeholder(tf.float32, [None, 32, 32, 3])      # 'None' means any number of rows (observations or batch_size)\n",
    "y_ph = tf.placeholder(tf.float32,[None, 10])\n",
    "drop_prob_ph = tf.placeholder(tf.float32)                 # The drop probability at the dropout layer is a hyperparameter.          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5. Define the Variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration of the first convolution layer is as following:\n",
    "- Kernel height = 7.\n",
    "- Kernel width = 7.\n",
    "- In_chanels = **3 (color)**. \n",
    "- Out_channels = 32 (number of feature maps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need Variables with the folllowing shapes:\n",
    "- Shape of the weight matrix = [kernel_height, kernel_width, in_channels, out_channels].\n",
    "- Shape of the bias = [out_channels]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables are defined according to the specifications mentioned above.\n",
    "W1 = tf.Variable(initial_value=tf.random_normal([7,7,3,32], mean=0, stddev=0.1))\n",
    "b1 = tf.Variable(initial_value=tf.fill([32], 0.1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration of the second convolution layer is as following: \n",
    "- Kernel height = 7.\n",
    "- Kernel width = 7.\n",
    "- In_chanels = 32 (out_channels from the previous convolution layer). \n",
    "- Out_channels = 64 (number of feature maps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need Variables with the folllowing shapes:\n",
    "- Shape of the weight matrix = [kernel_height, kernel_width, in_channels, out_channels].\n",
    "- Shape of the bias = [out_channels]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables are defined according to the specifications mentioned above.\n",
    "W2 = tf.Variable(initial_value=tf.random_normal([7,7,32,64], mean=0, stddev=0.1))\n",
    "b2 = tf.Variable(initial_value=tf.fill([64], 0.1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the following considerations for the flattened fully connected layer:\n",
    "- We will apply convolution twice with padding and there will be no image size reduction. \n",
    "- We will also apply max pooling twice with stride = 2 (vertically and horizontally). \n",
    "- At each max pooling with stride = 2, the image size is halved. Thus, **(32/2)/2 = 8** will be the size (vertical and horizontal) of the resulting final image.   \n",
    "- In the previous layer there were 64 output channels (feature maps). \n",
    "- Considering all these facts, there should be **8x8x64 = 4096** nodes in the flattened layer. \n",
    "- Finally, we will shrink the output from this layer to 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables are defined according to the specifications mentioned above.\n",
    "W3 = tf.Variable(initial_value=tf.random_normal([4096,1024], mean=0, stddev=0.1))\n",
    "b3 = tf.Variable(initial_value=tf.fill([1024], 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the following considerations for the final output layer: \n",
    "- There are 1024 nodes to match with the output from the previous layer.\n",
    "- We should shrink the output once more because there are 10 different labels (digits 0~9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables are defined according to the specifications mentioned above.\n",
    "W4 = tf.Variable(initial_value=tf.random_normal([1024,10], mean=0, stddev=0.1))\n",
    "b4 = tf.Variable(initial_value=tf.fill([10], 0.1))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6. Define the deep learning model (CNN): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the arguments:\n",
    "- padding = 'SAME' to apply a padding. padding = 'VALID' to apply no padding. \n",
    "- ksize = [1, kernel_height, kernel_width, 1]\n",
    "- strides = [1, stride_vertical, stride_horizontal,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Convolution layer\n",
    "y1 = tf.nn.conv2d(X_ph, W1, strides=[1, 1, 1, 1], padding='SAME') + b1\n",
    "conv1 = tf.nn.relu(y1)                             # Apply the ReLu activation function. \n",
    "# 1st Pooling layer\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd Convolution layer\n",
    "y2 = tf.nn.conv2d(pool1, W2, strides=[1, 1, 1, 1], padding='SAME') + b2\n",
    "conv2 = tf.nn.relu(y2)                            # Apply the ReLu activation function. \n",
    "# 2nd Pooling layer\n",
    "pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattened full layer\n",
    "conv2_flattened = tf.reshape(pool2, [-1,4096])   # 8x8x64 = 4096   \n",
    "y3 = tf.matmul(conv2_flattened, W3) + b3    \n",
    "full_layer = tf.nn.relu(y3)                      # Apply the ReLu activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout layer\n",
    "dropout_layer = tf.nn.dropout(full_layer, rate = drop_prob_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer\n",
    "y_model = tf.matmul(dropout_layer, W4) + b4      # No activation function. Softmax at the output layer is optional. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7. Define the loss function and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_ph, logits=y_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate = learn_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.8. Training and Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for i in range(n_epochs):\n",
    "            idx_rnd = np.random.choice(range(n_train_size),batch_size,replace=False)                         # Random sampling w/o replacement for the batch indices.                \n",
    "            batch_X, batch_y = X_train[idx_rnd,:,:] , y_train[idx_rnd]                                       # Sample a batch!\n",
    "            my_feed = {X_ph:batch_X, y_ph:batch_y, drop_prob_ph:drop_prob}\n",
    "            sess.run(train, feed_dict = my_feed)\n",
    "            if i % 500 == 0:            \n",
    "                correct_predictions = tf.equal(tf.argmax(y_ph, axis=1), tf.argmax(y_model, axis=1))          # In argmax(), axis=1 means horizontal direction.\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))                          # Recast the Boolean as float32 first. Then calculate the mean.\n",
    "                my_feed = {X_ph:X_test, y_ph:y_test, drop_prob_ph:0.0}                                       # No dropout for testing.\n",
    "                accuracy_value = sess.run(accuracy, feed_dict = my_feed)                              \n",
    "                print(\"Step = {}   ,   Accuracy = {:5.3f} \\n\".format(i, accuracy_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
